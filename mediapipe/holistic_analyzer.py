#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ì¢…í•© ì¸ê°„ í–‰ë™ ë¶„ì„ ì‹œìŠ¤í…œ - ì–¼êµ´, ì†, ìì„¸ë¥¼ ë™ì‹œì— ë¶„ì„
import cv2 # OpenCV ë¼ì´ë¸ŒëŸ¬ë¦¬, ë¹„ë””ì˜¤ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
import mediapipe as mp # MediaPipe ë¼ì´ë¸ŒëŸ¬ë¦¬, ì „ì²´ì ì¸(holistic) ì¶”ì ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
import numpy as np # NumPy ë¼ì´ë¸ŒëŸ¬ë¦¬, ìˆ˜ì¹˜ ê³„ì‚°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
import time # ì‹œê°„ ê´€ë ¨ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

# MediaPipe ì´ˆê¸°í™”
mp_holistic = mp.solutions.holistic # MediaPipeì˜ Holistic ì†”ë£¨ì…˜ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
mp_drawing = mp.solutions.drawing_utils # MediaPipeì˜ ê·¸ë¦¬ê¸° ê´€ë ¨ ìœ í‹¸ë¦¬í‹°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
mp_drawing_styles = mp.solutions.drawing_styles # MediaPipeì˜ ê·¸ë¦¬ê¸° ìŠ¤íƒ€ì¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

class HolisticAnalyzer: # ì „ì²´ì ì¸ ë¶„ì„ì„ ìœ„í•œ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    def __init__(self):
        self.start_time = time.time() # ë¶„ì„ ì‹œì‘ ì‹œê°„ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
        self.analysis_history = [] # ë¶„ì„ ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        
    def analyze_face_expression(self, face_landmarks):
        """ì–¼êµ´ í‘œì • ë¶„ì„"""
        if not face_landmarks: # ì–¼êµ´ ëœë“œë§ˆí¬ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìœ¼ë©´
            return "ì–¼êµ´ ì¸ì‹ ì•ˆë¨", 0.0
            
        # ì…ê¼¬ë¦¬ ìœ„ì¹˜ ë¶„ì„ (ë¯¸ì†Œ/ì°¡ê·¸ë¦¼)
        left_mouth = face_landmarks.landmark[61] # ì™¼ìª½ ì…ê¼¬ë¦¬
        right_mouth = face_landmarks.landmark[291] # ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬
        mouth_center = face_landmarks.landmark[13] # ì… ì¤‘ì•™ ì•„ë˜
        
        # ì–‘ìª½ ì…ê¼¬ë¦¬ì˜ yì¢Œí‘œ í‰ê· ê³¼ ì… ì¤‘ì•™ì˜ yì¢Œí‘œë¥¼ ë¹„êµí•˜ì—¬ ì…ì˜ ê³¡ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        mouth_curve = ((left_mouth.y + right_mouth.y) / 2) - mouth_center.y
        
        # ëˆˆ í¬ê¸° ë¶„ì„ (ë†€ëŒ/ì¡¸ë¦¼)
        left_eye_top = face_landmarks.landmark[159] # ì™¼ìª½ ëˆˆ ìœ„ìª½
        left_eye_bottom = face_landmarks.landmark[145] # ì™¼ìª½ ëˆˆ ì•„ë˜ìª½
        eye_openness = abs(left_eye_top.y - left_eye_bottom.y) # ëˆˆì„ ëœ¬ ì •ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        # í‘œì • íŒë³„
        if mouth_curve < -0.008: # ì…ê¼¬ë¦¬ê°€ ë§ì´ ì˜¬ë¼ê°”ìœ¼ë©´ (ë¯¸ì†Œ)
            return "Smile", abs(mouth_curve) * 100
        elif mouth_curve > 0.008: # ì…ê¼¬ë¦¬ê°€ ë§ì´ ë‚´ë ¤ê°”ìœ¼ë©´ (ì°¡ê·¸ë¦¼)
            return "Twist", mouth_curve * 100
        elif eye_openness > 0.02: # ëˆˆì´ í¬ê²Œ ë– ì¡Œìœ¼ë©´ (ë†€ëŒ)
            return "Surprise", eye_openness * 50
        else: # ê·¸ ì™¸ì˜ ê²½ìš°
            return "Normal", 0.5

    def analyze_hand_state(self, left_hand, right_hand):
        """ì–‘ì† ìƒíƒœ ë¶„ì„"""
        hand_states = [] # ì–‘ì†ì˜ ìƒíƒœë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        
        # ì™¼ìª½ ì†ê³¼ ì˜¤ë¥¸ìª½ ì†ì— ëŒ€í•´ ê°ê° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        for hand_landmarks, hand_name in [(left_hand, "ì™¼ì†"), (right_hand, "ì˜¤ë¥¸ì†")]:
            if hand_landmarks: # ì† ëœë“œë§ˆí¬ê°€ ê°ì§€ë˜ì—ˆìœ¼ë©´
                # í´ì§„ ì†ê°€ë½ì˜ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
                fingers = self.count_fingers(hand_landmarks)
                
                # ì œìŠ¤ì²˜ ë¶„ì„
                if fingers == 0:
                    gesture = "ì£¼ë¨¹ âœŠ"
                elif fingers == 5:
                    gesture = "ì—´ë¦° ì† ğŸ–ï¸"
                elif fingers == 2:
                    # ê²€ì§€ì™€ ì¤‘ì§€ê°€ í´ì ¸ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ 'í‰í™”' ì œìŠ¤ì²˜ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
                    index_up = hand_landmarks.landmark[8].y < hand_landmarks.landmark[6].y
                    middle_up = hand_landmarks.landmark[12].y < hand_landmarks.landmark[10].y
                    if index_up and middle_up:
                        gesture = "í‰í™” âœŒï¸"
                    else:
                        gesture = f"{fingers}ê°œ ì†ê°€ë½"
                elif fingers == 1:
                    gesture = "ê°€ë¦¬í‚¤ê¸° ğŸ‘†"
                else:
                    gesture = f"{fingers}ê°œ ì†ê°€ë½"
                
                hand_states.append(f"{hand_name}: {gesture}")
            else: # ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìœ¼ë©´
                hand_states.append(f"{hand_name}: ì¸ì‹ ì•ˆë¨")
        
        return hand_states # ë¶„ì„ëœ ì–‘ì†ì˜ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    def count_fingers(self, hand_landmarks):
        """ì†ê°€ë½ ê°œìˆ˜ ì„¸ê¸°"""
        finger_tips = [4, 8, 12, 16, 20]  # ì—„ì§€, ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ì†Œì§€ ëì  ì¸ë±ìŠ¤
        finger_pips = [3, 6, 10, 14, 18]  # ê° ì†ê°€ë½ì˜ ì¤‘ê°„ ê´€ì ˆ(PIP) ì¸ë±ìŠ¤
        
        fingers = 0 # í´ì§„ ì†ê°€ë½ ìˆ˜ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        
        # ì—„ì§€ëŠ” x ì¢Œí‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í´ì§/ì ‘í˜ì„ íŒë‹¨í•©ë‹ˆë‹¤.
        if hand_landmarks.landmark[4].x > hand_landmarks.landmark[3].x:
            fingers += 1
            
        # ë‚˜ë¨¸ì§€ ì†ê°€ë½ë“¤ì€ y ì¢Œí‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤. (ì†ê°€ë½ ëì´ ì¤‘ê°„ ê´€ì ˆë³´ë‹¤ ìœ„ì— ìˆìœ¼ë©´ í´ì§„ ê²ƒ)
        for tip, pip in zip(finger_tips[1:], finger_pips[1:]):
            if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[pip].y:
                fingers += 1
                
        return fingers # í´ì§„ ì†ê°€ë½ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    def analyze_body_posture(self, pose_landmarks):
        """ì‹ ì²´ ìì„¸ ë¶„ì„"""
        if not pose_landmarks: # ìì„¸ ëœë“œë§ˆí¬ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìœ¼ë©´
            return "ìì„¸ ì¸ì‹ ì•ˆë¨", []
            
        landmarks = pose_landmarks.landmark
        
        # ì–´ê¹¨ ìˆ˜í‰ í™•ì¸
        left_shoulder = landmarks[mp_holistic.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[mp_holistic.PoseLandmark.RIGHT_SHOULDER.value]
        shoulder_tilt = abs(left_shoulder.y - right_shoulder.y) # ì–‘ ì–´ê¹¨ì˜ yì¢Œí‘œ ì°¨ì´ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        # ë¨¸ë¦¬ ìœ„ì¹˜ í™•ì¸
        nose = landmarks[mp_holistic.PoseLandmark.NOSE.value]
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2 # ì–‘ ì–´ê¹¨ì˜ ì¤‘ì‹¬ xì¢Œí‘œ
        head_lean = abs(nose.x - shoulder_center_x) # ì½”ì™€ ì–´ê¹¨ ì¤‘ì‹¬ì˜ xì¢Œí‘œ ì°¨ì´ë¡œ ë¨¸ë¦¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        # ì•‰ìŒ/ì„œìˆìŒ íŒë³„
        hip_y = (landmarks[mp_holistic.PoseLandmark.LEFT_HIP.value].y + 
                landmarks[mp_holistic.PoseLandmark.RIGHT_HIP.value].y) / 2 # ì–‘ ì—‰ë©ì´ì˜ yì¢Œí‘œ í‰ê· 
        shoulder_y = (left_shoulder.y + right_shoulder.y) / 2 # ì–‘ ì–´ê¹¨ì˜ yì¢Œí‘œ í‰ê· 
        
        if abs(hip_y - shoulder_y) < 0.25: # ì–´ê¹¨ì™€ ì—‰ë©ì´ì˜ yì¢Œí‘œ ì°¨ì´ê°€ ì‘ìœ¼ë©´ ì•‰ì€ ìì„¸ë¡œ íŒë‹¨
            posture_type = "ì•‰ì€ ìì„¸"
        else:
            posture_type = "ì„  ìì„¸"
        
        # ìì„¸ í‰ê°€
        posture_issues = [] # ìì„¸ ë¬¸ì œì ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        if shoulder_tilt > 0.05: # ì–´ê¹¨ ê¸°ìš¸ê¸°ê°€ íŠ¹ì • ì„ê³„ê°’ë³´ë‹¤ í¬ë©´
            posture_issues.append("ì–´ê¹¨ ê¸°ìš¸ì–´ì§")
        if head_lean > 0.08: # ë¨¸ë¦¬ ê¸°ìš¸ê¸°ê°€ íŠ¹ì • ì„ê³„ê°’ë³´ë‹¤ í¬ë©´
            posture_issues.append("ë¨¸ë¦¬ ê¸°ìš¸ì–´ì§")
            
        return posture_type, posture_issues # ìì„¸ ìœ í˜•ê³¼ ë¬¸ì œì ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    def get_overall_assessment(self, face_emotion, hand_states, posture_type, posture_issues):
        """ì¢…í•© í‰ê°€"""
        assessment = [] # ì¢…í•© í‰ê°€ ë‚´ìš©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # ê°ì • ìƒíƒœ í‰ê°€
        if "ë¯¸ì†Œ" in face_emotion:
            assessment.append("ğŸ˜Š ê¸ì •ì ì¸ ê°ì • ìƒíƒœ")
        elif "ì°¡ê·¸ë¦¼" in face_emotion:
            assessment.append("ğŸ˜ ìŠ¤íŠ¸ë ˆìŠ¤ ìƒíƒœì¼ ìˆ˜ ìˆìŒ")
        else:
            assessment.append("ğŸ˜ í‰ì˜¨í•œ ê°ì • ìƒíƒœ")
            
        # ìì„¸ ìƒíƒœ í‰ê°€
        if len(posture_issues) == 0: # ë¬¸ì œì ì´ ì—†ìœ¼ë©´
            assessment.append("âœ… ì¢‹ì€ ìì„¸ ìœ ì§€ ì¤‘")
        else: # ë¬¸ì œì ì´ ìˆìœ¼ë©´
            assessment.append(f"âš ï¸ ìì„¸ ì£¼ì˜: {', '.join(posture_issues)}")
            
        # ì† í™œë™ì„± í‰ê°€
        active_hands = sum(1 for state in hand_states if "ì¸ì‹ ì•ˆë¨" not in state) # ì¸ì‹ëœ ì†ì˜ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
        if active_hands == 2:
            assessment.append("ğŸ¤² ì–‘ì† í™œë™ í™œë°œ")
        elif active_hands == 1:
            assessment.append("ğŸ‘‹ í•œì† í™œë™ ì¤‘")
        else:
            assessment.append("ğŸ¤ ì† í™œë™ ì—†ìŒ")
            
        return assessment # ì¢…í•© í‰ê°€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

# ì›¹ìº  ì‹œì‘
cap = cv2.VideoCapture(0) # 0ë²ˆ ì¹´ë©”ë¼(ê¸°ë³¸ ì›¹ìº )ë¥¼ ì—½ë‹ˆë‹¤.
analyzer = HolisticAnalyzer() # HolisticAnalyzer í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

# MediaPipe Holistic ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
with mp_holistic.Holistic(
    min_detection_confidence=0.5, # ìµœì†Œ ê°ì§€ ì‹ ë¢°ë„
    min_tracking_confidence=0.5) as holistic:
    
    while cap.isOpened(): # ì›¹ìº ì´ ì—´ë ¤ ìˆëŠ” ë™ì•ˆ ê³„ì† ë°˜ë³µí•©ë‹ˆë‹¤.
        ret, frame = cap.read() # ì›¹ìº ì—ì„œ í•œ í”„ë ˆì„ì„ ì½ì–´ì˜µë‹ˆë‹¤.
        if not ret: # í”„ë ˆì„ì„ ì œëŒ€ë¡œ ì½ì–´ì˜¤ì§€ ëª»í–ˆë‹¤ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
            break

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # ì´ë¯¸ì§€ë¥¼ BGRì—ì„œ RGBë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        image.flags.writeable = False # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì´ë¯¸ì§€ì— ì“°ê¸° ì‘ì—…ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        
        # ì „ì²´ ë¶„ì„
        results = holistic.process(image) # Holistic ëª¨ë¸ë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì–¼êµ´, ì†, ìì„¸ ëœë“œë§ˆí¬ë¥¼ ì–»ìŠµë‹ˆë‹¤.
        
        # ì´ë¯¸ì§€ ë‹¤ì‹œ ë³€í™˜
        image.flags.writeable = True # í™”ë©´ì— ê·¸ë¦¬ê¸° ìœ„í•´ ì“°ê¸° ì‘ì—…ì„ ë‹¤ì‹œ í™œì„±í™”í•©ë‹ˆë‹¤.
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ RGBì—ì„œ BGRë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        # ê° ì˜ì—­ë³„ ë¶„ì„
        face_emotion, emotion_score = analyzer.analyze_face_expression(results.face_landmarks)
        hand_states = analyzer.analyze_hand_state(results.left_hand_landmarks, results.right_hand_landmarks)
        posture_type, posture_issues = analyzer.analyze_body_posture(results.pose_landmarks)
        
        # ì¢…í•© í‰ê°€
        overall_assessment = analyzer.get_overall_assessment(
            face_emotion, hand_states, posture_type, posture_issues)
        
        # ê²°ê³¼ í‘œì‹œ
        # ì •ë³´ í‘œì‹œë¥¼ ìœ„í•œ ë°˜íˆ¬ëª… ë°°ê²½ ì˜ì—­ì„ ìƒì„±í•©ë‹ˆë‹¤.
        overlay = image.copy()
        cv2.rectangle(overlay, (10, 10), (400, 200), (0, 0, 0), -1)
        image = cv2.addWeighted(overlay, 0.7, image, 0.3, 0)
        
        y_pos = 30 # í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•  y ì‹œì‘ ìœ„ì¹˜
        cv2.putText(image, f"ì–¼êµ´: {face_emotion}", (20, y_pos), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2) # ì–¼êµ´ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        
        y_pos += 25
        for hand_state in hand_states: # ì† ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            cv2.putText(image, hand_state, (20, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            y_pos += 20
            
        y_pos += 5
        cv2.putText(image, f"ìì„¸: {posture_type}", (20, y_pos), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1) # ìì„¸ ìœ í˜• í‘œì‹œ
        
        if posture_issues: # ìì„¸ ë¬¸ì œì ì´ ìˆìœ¼ë©´
            y_pos += 20
            cv2.putText(image, f"ì£¼ì˜: {', '.join(posture_issues)}", (20, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 255), 1) # ë¬¸ì œì  ë‚´ìš© í‘œì‹œ
        
        # ì¢…í•© í‰ê°€ í‘œì‹œ
        y_pos = 250
        cv2.putText(image, "=== ì¢…í•© í‰ê°€ ===", (20, y_pos), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        for assessment in overall_assessment:
            y_pos += 25
            cv2.putText(image, assessment, (20, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1) # ì¢…í•© í‰ê°€ ë‚´ìš© í‘œì‹œ
        
        # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        # ì–¼êµ´ ëœë“œë§ˆí¬(ìœ¤ê³½ì„ )ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        if results.face_landmarks:
            mp_drawing.draw_landmarks(
                image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,
                None, mp_drawing_styles.get_default_face_mesh_contours_style())
        
        # ì™¼ìª½ ì† ëœë“œë§ˆí¬ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        if results.left_hand_landmarks:
            mp_drawing.draw_landmarks(
                image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())
        
        # ì˜¤ë¥¸ìª½ ì† ëœë“œë§ˆí¬ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        if results.right_hand_landmarks:
            mp_drawing.draw_landmarks(
                image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())
        
        # ìì„¸ ëœë“œë§ˆí¬ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        if results.pose_landmarks:
            mp_drawing.draw_landmarks(
                image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                mp_drawing_styles.get_default_pose_landmarks_style())
        
        cv2.imshow('Holistic Human Behavior Analysis', image) # ìµœì¢… ì´ë¯¸ì§€ë¥¼ ì°½ì— ë³´ì—¬ì¤ë‹ˆë‹¤.
        
        if cv2.waitKey(10) & 0xFF == ord('q'): # 'q' í‚¤ê°€ ëˆŒë¦¬ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
            break

cap.release() # ì‚¬ìš©ì´ ëë‚œ ì›¹ìº  ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•©ë‹ˆë‹¤.
cv2.destroyAllWindows() # ëª¨ë“  OpenCV ì°½ì„ ë‹«ìŠµë‹ˆë‹¤.